# ğŸ“‰ Telco Customer Churn Prediction Pipeline

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688)
![Docker](https://img.shields.io/badge/Docker-Enabled-2496ED)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Model-F7931E)
![License](https://img.shields.io/badge/License-MIT-green)

> A production-ready End-to-End Machine Learning project that predicts customer churn using a modular pipeline, served via a REST API.

## ğŸ“– Overview

This project demonstrates a professional MLOps workflow for a binary classification problem (Churn Prediction). Instead of a standalone notebook, it implements a full software lifecycle including:
- **Reproducible Data Pipeline**: Automated ingestion and cleaning.
- **Robust Preprocessing**: Scikit-Learn pipelines for feature engineering.
- **Model Serving**: High-performance API using FastAPI.
- **Containerization**: Fully Dockerized for easy deployment.
- **Testing**: Unit tests for data validity and API endpoints.

## ğŸš€ Key Features

- **Production-Grade Architecture**: Code is organized into modular components (`src`, `app`, `tests`) rather than monolithic scripts.
- **Automated Data Ingestion**: System automatically downloads the [Telco Customer Churn](https://www.kaggle.com/blastchar/telco-customer-churn) dataset if missing.
- **Type-Safe API**: Uses **Pydantic** schemas for strict input validation, preventing bad data from hitting the model.
- **Interactive Documentation**: Integrated Swagger UI for testing the API in real-time.
- **Docker Integration**: `docker-compose` setup for one-command deployment.
- **CI/CD Ready**: Structure supports easy integration with GitHub Actions (tests included).

## ğŸ› ï¸ Tech Stack

- **Model**: Scikit-Learn (Random Forest)
- **API**: FastAPI, Uvicorn
- **Data**: Pandas, NumPy
- **Containerization**: Docker, Docker Compose
- **Testing**: Pytest

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[Data Source] -->|Ingest| B[Data Loader]
    B -->|Raw Data| C[Preprocessing Pipeline]
    C -->|Features| D[Random Forest Model]
    D -->|Serialized Artifacts| E[Model Registry]
    E -->|Load| F[FastAPI Service]
    F -->|Serve| G[Prediction Endpoint]
    G -->|JSON| H[Client / Recruiter]
```

## ğŸ“‚ Project Structure

```text
ml-pipeline-project/
â”œâ”€â”€ .github/             # GitHub configurations
â”œâ”€â”€ app/                 # FastAPI Application
â”‚   â”œâ”€â”€ main.py          # API Endpoints
â”‚   â””â”€â”€ schemas.py       # Pydantic Data Models
â”œâ”€â”€ data/                # Data storage (ignored by git)
â”œâ”€â”€ notebooks/           # EDA and Prototyping
â”œâ”€â”€ src/                 # Core ML Logic
â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”œâ”€â”€ data_loader.py   # Data ingestion
â”‚   â”œâ”€â”€ evaluate.py      # Model metrics
â”‚   â”œâ”€â”€ preprocessing.py # Feature engineering pipeline
â”‚   â””â”€â”€ train.py         # Training script
â”œâ”€â”€ tests/               # Unit Tests
â”œâ”€â”€ Dockerfile           # Docker image definition
â”œâ”€â”€ docker-compose.yml   # Docker services config
â””â”€â”€ requirements.txt     # Python dependencies
```

## ğŸ’» Installation & Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/SrikarK21/telco-churn-ml.git
   cd telco-churn-ml
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   ```

3. **Activate & Install Dependencies**
   *Windows:*
   ```powershell
   .\venv\Scripts\activate
   pip install -r requirements.txt
   ```
   *Linux/Mac:*
   ```bash
   source venv/bin/activate
   pip install -r requirements.txt
   ```

   > **Note for Windows Users**: If you encounter a "scripts is disabled" error, run:
   > `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass`

## ğŸƒ Usage

### 1. Train the Model
This will download data, preprocess it, train the Random Forest, and save the model to `models/model.joblib`.
```bash
python src/train.py
```
*(Windows users can also double-click `run_train.bat`)*

### 2. Start the API Server
Launch the FastAPI server locally.
```bash
uvicorn app.main:app --reload
```
*(Windows users can also double-click `run_api.bat`)*

### 3. Get Predictions
Open your browser to **[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)**.
- Click **"POST /predict"** -> **"Try it out"**.
- Edit the customer JSON data.
- Click **"Execute"** to see the probability of churn.

## ğŸ³ Docker Deployment

To run the entire system in an isolated container:
```bash
docker-compose up --build
```
The API will be available at `http://localhost:8000`.

## ğŸ§ª Testing

Run the test suite to verify data pipelines and API responses:
```bash
pytest tests/
```

## ğŸ“Š Dataset
The dataset used is the [Telco Customer Churn](https://www.kaggle.com/blastchar/telco-customer-churn) dataset from Kaggle.
